# POPL 2022 Artifact Evaluation instructions

We were unable to get a VirtualBox VM running under macOS Big Sur, so we provide a Docker-based artifact instead.

## List of supported claims

The artifact consists of an implementation, in PureScript, of the programming language described in the paper (called _Fluid_ in the non-anonymised submission). More specifically, the artifact implements:
- the core language described in § 2;
- the bidirectional analysis described in § 3, and its De Morgan dual as described in § 4;
- the surface language described in § 5.

The specific claims supported by the artifact relate to the following 4 figures in the paper.

### Figure 1

[§ 4, second sentence] "In particular [forward evaluation] can answer questions like: “what data is needed to compute this bar in a bar chart?”, and indeed **we were able to use our implementation to generate Figure 1**." The function ```linkingFigs``` in `src\app\Demo.purs` generates 6 output images; 4 of these were combined to produce Figure 1. See Evaluation Step 1.

### Figure 2

[§ 4.1, final sentence] "This is the approach implemented in Fluid, and **we used this to generate Figure 2** in § 1.2". Of the 6 images produced by ```linkingFigs```, 3 were combined to produce Figure 2. See Evaluation Step 2.

### Figure 13

[§ 4.2, paragraph 2] "**Fluid was used to generate the diagrams in Figure 13**..." The function ```convolutionFigs``` in `src\app\Demo.purs` generates 9 output images which were combined to produce Figure. 13. See Evaluation Step 3.

### Figure 16

[§ 5.1, paragraph 1] "**Figure 16 shows how the end-to-end mapping would appear to a user.**" The function ```testBwd``` of `src\test\Main.purs` runs the test `section-5-example`, which generates the raw information required for Figure 16. See Evaluation Step 4.

## Download, installation, and sanity-testing

### Software required

- git
- Docker; we have tested with Docker Desktop 4.1.0 for macOS

### Obtaining the artifact

Follow the following steps:
- `git clone https://github.com/explorable-viz/fluid.git` to get this repo
- checkout tag `v0.4.0`
- ensure Docker service is running
- in the repository root, run `docker build -t fluid-webapp .` to build Docker image

### How to run the artifact

The majority of the evaluation outputs are generated by the web app; two are generated by the test suite.

#### Running the web app

To run the web app:
- `docker run -p 1234:1234 -it fluid-webapp` to start web server in Docker container
- open a browser (preferably Chrome) at `http://localhost:1234/`.

Some text will load (including a heading saying "Fluid 0.4"); if everything is working correctly, the images required for Evaluation Steps 1-3 will load after about 30 seconds.

#### Running the test suite

Alternatively, to get a shell prompt from which you can run tests:
- `docker run -it --entrypoint=/bin/bash fluid-webapp` to shell into the container

Then sanity-check that the sources compile cleanly with:
- `yarn run clean-tests`
- `yarn build-tests`

This should report 0 warnings, 0 errors, and conclude with "build suceeded". The compiled output is written to `dist/test/app.js`.

Finally, verify the tests with:

- `yarn run tests` to run the tests in Chrome headless mode

56 tests should pass in about 4 minutes. The tests serve to validate the implementation as a whole, as well as generating the raw output for Evaluation Step 4, as described below.

#### Running individual tests

Running an individual test takes a small amount of manual effort: one must replace the top-level invocation of the test suite by a call to a predefined function called `test_scratchpad`, which runs a list of specific tests. Specifically:

* in `test/Main.purs`, in the first function of the file (`tests`), comment out `tests = [ test_desugaring, test_misc, test_bwd, test_linking, test_graphics ]` and
uncomment `tests = [ test_scratchpad ]`
* insert the code for the test you want to run into the list in `test_scratchpad`; initially that will contain a `zipWith` test, by way of an example
* then run the whole test suite, using `yarn run tests`  as described above

## Evaluation instructions

### Step 1

The raw ingredients for Figure 1 are served by the web app as part of a web page. To verify, follow the instructions above for running the web app. The web page `http://localhost:1234/` should then display three bar charts; the first and penultimate of these correspond to the two bar charts in Fig. 1. The two tabular views correspond to the two differently-highlighted parts of the table in the middle of Fig 1.  The relevant PureScript code is the function ```linkingFigs``` in `src/app/Demo.purs`, which loads the Fluid source code for the bar chart, shown at the bottom of Fig. 2. This can be found in `fluid/example/linking/bar-chart.fld`.

We made the following manual changes to the images for inclusion in the paper: we added the "selection 1" and "selection 2" arrows; we changed the selection color on the right-hand side from orange to blue; and we combined the two tabular views into a single table.

### Step 2

The raw ingredients for Figure 2 are also generated by the web app, which we assume has already been built using the instructions above. The first three images displayed on the web page `http://localhost:1234/` were combined to form Figure 2. The relevant PureScript code is the function ```linkingFigs``` in `src/app/Demo.purs`, which as well as loading the bar chart code (as described in from Step 1) also loads the Fluid code for the line chart shown at the bottom of Fig. 2. This can be found in `fluid/example/linking/line-chart.fld`.

We made the following manual changes to the images for inclusion in the paper: we added the "what data to do I need?" and "what needs this data?" arrows.

### Step 3

The raw ingredients for Figure 13 are also generated by the web app, which we assume has already been built using the instructions above. The matrices shown in the lower half of `http://localhost:1234/` contain the raw ingredients of Fig. 13; the first row of matrices, headed "Needs relation (and round-trip)", were combined to produce Fig. 13a; the second row of matrices, headed "Needed-by relation (and round-trip)", were combined to produce Fig. 13b.

The Fluid source code used for this example can be found in `fluid/lib/convolution.fld` and `fluid/example/slicing/conv-emboss.fld`. This example is also verified by the function `test_bwd` in `test/Main.purs`, which runs the test case `fluid/example/slicing/conv-emboss.fld` with the output expectation `conv-emboss.expect.fld`, found in the same folder. This can be run by following the instructions above for running the test suite.

We made the following manual changes to the images for inclusion in the paper: we added the "what do I need?", "what only needs me?", "what needs me?", and "what do only I need?" arrows; the vertical bars separating inputs and outputs; and the headings "input selection A", "output selection B", etc.

### Step 4

The raw ingredients for Figure 16 (see above) are generated by the test function ```testBwd``` of `src/test/Main.purs`, running the test called `section-5-example`. To verify, follow the instructions above for running the test suite. The relevant Fluid source file is `fluid/example/slicing/section-5-example.fld`, with the output expectations in `fluid/example/slicing/section-5-example-1.expect.fld` (for left-hand side of Fig. 16) and `fluid/example/slicing/section-5-example-2.expect.fld` (for right-hand side of Fig. 16). In these and other test expectations, underscores are used to indicate "selected" data values.

We made the following manual changes to the images for inclusion in the paper: we removed the underscores indicating selections and used appropriately shaded backgrounds instead.

## Additional artifact description

The Fluid source code used for the tests and web app are found in the `fluid/example` directory. The core library is found in `lib/prelude`, with the matrix convolution functions in `lib/convolution`. The dataset used for the linking examples is in the folder `fluid/dataset`. Fluid test files have the extension `.fld`; the examples in the `fluid/slicing` folder also come with `.expect.fld` files, which capture the expected selection state on the program that arises from a backward slice.

Reviewers may wish to experiment with different Fluid source files and test expectations; we would be happy to assist with this.