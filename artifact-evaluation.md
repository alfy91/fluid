# POPL 2022 Artifact Evaluation instructions

We were unable to get a VirtualBox VM running under macOS Big Sur, so we provide a Docker-based artifact instead.

## List of supported claims

The artifact consists of an implementation, in PureScript, of the programming language described in the paper (called _Fluid_ in the non-anonymised submission). More specifically, the artifact implements:
- the core language described in § 2;
- the bidirectional analysis described in § 3, and its De Morgan dual as described in § 4;
- the surface language described in § 5.

The specific claims supported by the artifact relate to the following 4 figures in the paper.

### Figure 1

[§ 4, second sentence] "In particular [forward evaluation] can answer questions like: “what data is needed to compute this bar in a bar chart?”, and indeed **we were able to use our implementation to generate Figure 1**." The function ```linkingFigs``` in `src\app\Demo.purs` generates 6 output images; 4 of these were combined to produce Figure 1. See Evaluation Step 1.

### Figure 2

[§ 4.1, final sentence] "This is the approach implemented in Fluid, and **we used this to generate Figure 2** in § 1.2". Of the 6 images produced by ```linkingFigs```, 3 were combined to produce Figure 2. See Evaluation Step 2.

### Figure 13

[§ 4.2, paragraph 2] "**Fluid was used to generate the diagrams in Figure 13**..." The function ```convolutionFigs``` in `src\app\Demo.purs` generates 9 output images which were combined to produce Figure. 13. See Evaluation Step 3.

### Figure 16

[§ 5.1, paragraph 1] "**Figure 16 shows how the end-to-end mapping would appear to a user.**" The function ```testBwd``` of `src\test\Main.purs` runs the test `section-5-example`, which generates the raw information required for Figure 16. See Evaluation Step 4.

## Download, installation, and sanity-testing

### Software required

- git
- Docker; we have tested with Docker Desktop 4.1.0 for macOS

### Obtaining the artifact

Follow the following steps:
- `git clone https://github.com/explorable-viz/fluid.git` to get this repo
- ensure Docker service is running
- in the repository root, run `docker build -t fluid-webapp .` to build Docker image

### How to run the artifact

The majority of the evaluation outputs are generated by the web app; two are generated by the test suite.

#### Running the web app

To run the web app:
- `docker run -p 1234:1234 -it fluid-webapp` to start web server in Docker container
- open a browser (preferably Chrome) at `http://localhost:1234/`.

Some text will load (including a heading saying "Fluid 0.4"); if everything is working correctly, the images required for Evaluation Steps 1-3 will load after about 30 seconds.

#### Running the tests

Alternatively, to get a shell prompt from which you can run tests:
- `docker run -it --entrypoint=/bin/bash fluid-webapp` to shell into the container

Then sanity-check that the sources compile cleanly with:
- `yarn run clean-tests`
- `yarn build-tests`

This should report 0 warnings, 0 errors, and conclude with "build suceeded". The compiled output is written to `dist/test/app.js`.

Finally, verify the tests with:

- `yarn run tests` to run the tests in Chrome headless mode

56 tests should pass in about 4 minutes. The tests serve to validate the implementation as a whole, as well as generating the raw output for Evaluation Step 4, as described below.

## Evaluation instructions

### Step 1

The raw ingredients for Figure 1 (see above) are served by the web app as part of a web page. To verify, follow the instructions above for running the web app. The web page `http://localhost:1234/` should then display several bar charts; the first and last of these correspond to the two bar charts in Fig. 1. The two tabular views correspond to the two tables in the middle of Fig 1.  The relevant PureScript code is the function ```linkingFigs``` in `src\app\Demo.purs`, which loads the Fluid source code for the bar chart (shown at the bottom of Fig. 2) from `fluid/example/linking/bar-chart.fld`.

We made the following manual changes to the images for inclusion in the paper: we added the "selection 1" and "selection 2" arrows; we changed the selection color on the right-hand side from orange to blue; and we combined the two tabular views into a single table.

### Step 2

The raw ingredients for Figure 1 (see above) are generated by the web app; we assume the web app has already been built using the instructions above. The first three images displayed on the web page `http://localhost:1234/` were combined to form Figure 2. The relevant PureScript code is the function ```linkingFigs``` in `src\app\Demo.purs`, which as well as loading the bar chart code (as described in from Step 1) also loads the Fluid code for the line chart (shown at the bottom of Fig. 2) from `fluid/example/linking/line-chart.fld`.

We made the following manual changes to the images for inclusion in the paper: we added the "what data to do I need?" and "what needs this data?" arrows.

### Step 3

The raw ingredients for Figure 13 (see above) are also generated by the web app; we assume the web app has already been built using the instructions above. The matrices shown on `http://localhost:1234/` contains the raw ingredients of Fig. 13; the first row of matrices, headed "Needs relation (and round-trip)", were combined to produce Fig. 13a; the second row of matrices, headed "Needed-by relation (and round-trip)", were combined to produce Fig. 13b.

The Fluid source code used for this example can be found in `fluid/lib/convolution.fld` and `fluid/example/slicing/conv-emboss.fld`. This example is also verified by the function `test_bwd` in `test/Main.purs`, which runs the test case `conv-emboss.fld` with the output expectation `fluid/example/slicing/conv-emboss.expect.fld`.

We made the following manual changes to the images for inclusion in the paper: we added the "what do I need?", "what only needs me?", "what needs me?", and "what do only I need?" arrows; the vertical bars separating inputs and outputs; and the headings "input selection A", "output selection B", etc.

### Step 4

The raw ingredients for Figure 16 (see above) are generated by the test function ```testBwd``` of `src\test\Main.purs`, running the test called `section-5-example`. To verify, follow the instructions above for running the tests inside a container shell. The relevant Fluid source file is `fluid/example/slicing/section-5-example.fld`, with the output expectations in `fluid/example/slicing/section-5-example-1.expect.fld` (for left-hand side of Fig. 16) and `fluid/example/slicing/section-5-example-2.expect.fld` (for right-hand side of Fig. 16). In these and other test expectations, underscores are used to indicate "selected" data values.

We made the following manual changes to the images for inclusion in the paper: we removed the underscores indicating selections and used appropriately shaded backgrounds instead.

## Additional artifact description

The Fluid source code used for the tests and web app are found in the `fluid/example` directory. The core library is found in `lib/prelude`, with the matrix convolution functions in `lib/convolution`. The dataset used for the linking examples is in the folder `fluid/dataset`.

Reviewers may wish to experiment with different Fluid source files and test expectations; we would be happy to assist with this.
